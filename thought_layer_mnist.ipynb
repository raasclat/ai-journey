{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdTlJfI3Z6wE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import random_split\n",
        "import random\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)  # Set seed for random module\n",
        "    np.random.seed(seed)  # Set seed for NumPy\n",
        "    torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU (if using CUDA)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior in PyTorch\n",
        "    torch.backends.cudnn.benchmark = False  # Disable benchmark mode for reproducibility\n",
        "\n",
        "# Set a specific seed value for reproducibility\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A9xTXtIZ6wI"
      },
      "outputs": [],
      "source": [
        "class ThoughtLayer(nn.Module):\n",
        "    def __init__(self, input_size, num_neurons):\n",
        "        super(ThoughtLayer, self).__init__()\n",
        "        # Initialize weights and biases for the neurons\n",
        "        self.weights = nn.Parameter(torch.randn(num_neurons, input_size))\n",
        "        self.biases = nn.Parameter(torch.randn(num_neurons))\n",
        "\n",
        "        # Initialize positions for neurons in 1D space (linear positions)\n",
        "        self.positions = nn.Parameter(torch.rand(num_neurons))  # 1D positions\n",
        "\n",
        "        # Parameters for influence and movement\n",
        "        self.activation_threshold = 0.8\n",
        "        self.learning_rate_position = 0.01\n",
        "        self.influence_radius = 0.5  # Radius for nearby neuron influence\n",
        "        self.influence_strength = 0.2\n",
        "        self.barnes_hut_theta = 0.7  # Larger value for faster approximation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input if it's coming from an image (batch_size, 1, 28, 28) to (batch_size, 784)\n",
        "        if len(x.shape) == 4:  # Check if input is 4D (batch_size, channels, height, width)\n",
        "            x = x.view(x.size(0), -1)  # Flatten to (batch_size, 784)\n",
        "\n",
        "        # Compute activations\n",
        "        weighted_sum = torch.matmul(x, self.weights.T) + self.biases\n",
        "        activations = torch.sigmoid(weighted_sum)\n",
        "\n",
        "        # Update positions for high-value neurons\n",
        "        self.update_positions(activations)\n",
        "\n",
        "        # Apply Barnes-Hut approximation to influence neurons\n",
        "        activations = self.influence_neurons(activations)\n",
        "\n",
        "        return activations\n",
        "\n",
        "    def update_positions(self, activations):\n",
        "        high_value_neurons = activations > self.activation_threshold\n",
        "        high_value_indices = high_value_neurons.nonzero(as_tuple=True)[0]\n",
        "\n",
        "        if len(high_value_indices) > 1:\n",
        "            # Vectorized average position update\n",
        "            avg_position = self.positions[high_value_indices].mean()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.positions[high_value_indices] -= self.learning_rate_position * (self.positions[high_value_indices] - avg_position)\n",
        "\n",
        "    def influence_neurons(self, activations):\n",
        "        # Vectorized distance calculation\n",
        "        batch_size = activations.size(0)\n",
        "        positions_expanded = self.positions.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        distances = torch.abs(positions_expanded.unsqueeze(2) - positions_expanded.unsqueeze(1))  # Shape: [batch_size, num_neurons, num_neurons]\n",
        "\n",
        "        # Apply the influence only to nearby neurons using a mask\n",
        "        nearby_mask = distances < self.influence_radius  # Boolean mask for nearby neurons\n",
        "        distant_mask = distances >= self.influence_radius * self.barnes_hut_theta\n",
        "\n",
        "        # Vectorized influence application for nearby neurons\n",
        "        nearby_distances = distances * nearby_mask\n",
        "        influence_nearby = self.influence_strength * (1 - nearby_distances.mean(dim=2) / self.influence_radius)\n",
        "\n",
        "        updated_activations = activations.clone() + influence_nearby\n",
        "\n",
        "        # Vectorized Barnes-Hut approximation for distant neurons\n",
        "        distant_distances = distances * distant_mask\n",
        "        distant_positions_mean = self.positions.unsqueeze(1).mean(dim=1)  # Approximate mean position of distant neurons\n",
        "        distant_influence = self.influence_strength * (1 - torch.abs(self.positions - distant_positions_mean) / self.influence_radius)\n",
        "\n",
        "        updated_activations += distant_influence\n",
        "\n",
        "        # Ensure activations stay in [0, 1] range\n",
        "        return torch.clamp(updated_activations, 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bnVz_MsZ6wL"
      },
      "outputs": [],
      "source": [
        "class ThoughtModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_custom_neurons):\n",
        "        super(ThoughtModel, self).__init__()\n",
        "\n",
        "        # First hidden layer (custom thought layer)\n",
        "        self.thought_layer = ThoughtLayer(input_size=input_size, num_neurons=num_custom_neurons)\n",
        "\n",
        "        # Second hidden layer (custom thought layer)\n",
        "        self.thought_layer2 = ThoughtLayer(input_size=num_custom_neurons, num_neurons=hidden_size)\n",
        "\n",
        "        # Output layer (fully connected)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.thought_layer(x)\n",
        "        x = torch.relu(self.thought_layer2(x))\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCUruEyHZ6wO"
      },
      "outputs": [],
      "source": [
        "#Define the transformations to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Create DataLoader objects for batching\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "# Assuming train_dataset is the dataset used in train_loader\n",
        "train_size = int(0.8 * len(train_dataset))  # 80% for training\n",
        "val_size = len(train_dataset) - train_size  # 20% for validation\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create new DataLoader for the validation dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCCCnspoZ6wQ"
      },
      "outputs": [],
      "source": [
        "# Initialize the model (make sure input/output sizes match the dataset)\n",
        "input_size = 784  # 784 features in the digits dataset\n",
        "hidden_size = 128  # You can adjust this\n",
        "output_size = 10  # Number of classes (10 for digits dataset)\n",
        "num_custom_neurons = 64  # Adjust as needed\n",
        "\n",
        "model = ThoughtModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_custom_neurons=num_custom_neurons)\n",
        "\n",
        "# Move model to the device\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # For classification tasks\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "collapsed": true,
        "id": "YfhfRf8BZ6wS",
        "outputId": "fa7ac513-55f1-4cca-b730-2dd72cbfaea0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FullyConnectedModel' object has no attribute 'thought_layer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e5484c478085>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Store neuron positions at the end of the epoch for both thought layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mall_positions_layer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthought_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mall_positions_layer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthought_layer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FullyConnectedModel' object has no attribute 'thought_layer'"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 30\n",
        "\n",
        "# Initialize lists to store training losses and accuracies\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "\n",
        "# Initialize lists to store positions of both thought layers\n",
        "all_positions_layer1 = []\n",
        "all_positions_layer2 = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move data to the device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average loss and accuracy for this epoch\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Store the metrics\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracies.append(accuracy)\n",
        "\n",
        "    # Store neuron positions at the end of the epoch for both thought layers\n",
        "    all_positions_layer1.append(model.thought_layer.positions.detach().cpu().numpy())\n",
        "    all_positions_layer2.append(model.thought_layer2.positions.detach().cpu().numpy())\n",
        "\n",
        "    epoch_time = time.time() - start_time  # End time\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Time per epoch: {epoch_time:.2f} seconds\")\n",
        "\n",
        "# After training, you can plot the losses and accuracies using matplotlib if needed\n",
        "\n",
        "# After training, you can save the model if desired\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k6WBviIBZ6wV"
      },
      "outputs": [],
      "source": [
        "# After initializing your training losses and accuracies\n",
        "val_losses = []  # Initialize the list for validation losses\n",
        "val_accuracies = []  # Initialize the list for validation accuracies\n",
        "\n",
        "# Evaluation phase\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "val_epoch_loss = 0.0\n",
        "val_correct = 0\n",
        "val_total = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute validation loss\n",
        "        val_loss = criterion(outputs, labels)\n",
        "        val_epoch_loss += val_loss.item()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        val_total += labels.size(0)\n",
        "        val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average validation loss and accuracy for this epoch\n",
        "avg_val_loss = val_epoch_loss / len(val_loader)\n",
        "val_accuracy = val_correct / val_total\n",
        "\n",
        "# Store the validation metrics\n",
        "val_losses.append(avg_val_loss)  # Use val_losses\n",
        "val_accuracies.append(val_accuracy)  # Use val_accuracies\n",
        "\n",
        "print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "# Print total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWXt2CMGZ6wW"
      },
      "outputs": [],
      "source": [
        "def plot_neuron_positions_1d(all_positions_layer1, all_positions_layer2):\n",
        "    num_epochs = len(all_positions_layer1)\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        plt.clf()  # Clear the plot before each epoch\n",
        "        positions_layer1 = all_positions_layer1[epoch]  # Neuron positions for layer 1\n",
        "        positions_layer2 = all_positions_layer2[epoch]  # Neuron positions for layer 2\n",
        "\n",
        "        # Plot neuron positions for both layers in 1D space\n",
        "        plt.scatter(positions_layer1, [0] * len(positions_layer1), c='b', marker='o', label='Layer 1')\n",
        "        plt.scatter(positions_layer2, [1] * len(positions_layer2), c='r', marker='x', label='Layer 2')\n",
        "\n",
        "        plt.title(f'Epoch {epoch + 1}/{num_epochs} - Neuron Positions for Both Layers')\n",
        "        plt.xlabel('Neuron Position (1D)')\n",
        "        plt.yticks([0, 1], ['Layer 1', 'Layer 2'])  # Separate y-axis values for the two layers\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.pause(0.5)  # Pause to visualize each epoch's neuron positions\n",
        "        plt.show()\n",
        "\n",
        "# Call the function to plot 1D neuron positions for both thought layers\n",
        "plot_neuron_positions_1d(all_positions_layer1, all_positions_layer2)\n",
        "\n",
        "# Plotting training and validation losses and accuracies\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plotting losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
        "plt.title('Loss per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Plotting accuracies\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='orange')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}